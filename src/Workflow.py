import struct
import time
import open3d as o3d
import pickle
import numpy as np
from src import Cluster
from src import Discontinuity
from src import PointCloud
import pandas as pd


#####################################################################################
#####################################################################################
#####################################################################################

def load_pointcloud_DSE(data_path):
    '''
    :param data_path: æ–‡ä»¶è·¯å¾„ï¼Œtabåˆ†éš”ã€æ— è¡¨å¤´
    :return: X, Y, Z, js, cl, A, B, C, Dï¼ˆä¸ºå¼ é‡åŒ–å­—æ®µï¼‰ï¼Œrock_pointcloud å¯¹è±¡
    '''
    starttime = time.perf_counter()
    # è¯»å–æ•°æ®ï¼ˆæ— è¡¨å¤´ï¼Œtab åˆ†éš”ï¼‰
    data = np.loadtxt(data_path, delimiter='\t')

    # æå–æ‰€æœ‰ (js, cl) å¹¶ç”Ÿæˆå”¯ä¸€ cluster_id
    js_cl_list = [(int(row[3]), int(row[4])) for row in data]
    unique_clusters = list(set(js_cl_list))
    cluster_id_dict = {key: idx for idx, key in enumerate(unique_clusters)}
    clusters_pointcloud = {key: PointCloud.RockPointCloud() for idx, key in enumerate(unique_clusters)}

    total_pointcloud = PointCloud.RockPointCloud()
    for point_id, row in enumerate(data):
        X, Y, Z = row[0:3]
        joint_id = int(row[3])
        joint_cluster_id = int(row[4])
        cluster_id = cluster_id_dict[(joint_id, joint_cluster_id)]
        R, G, B = 0, 0, 0
        a, b, c, d = row[5:9]

        total_pointcloud.add(X, Y, Z, point_id, joint_id, joint_cluster_id, cluster_id, R, G, B, a, b, c, d)

    # æå–æ„Ÿå…´è¶£å­—æ®µç”¨äºå¼ é‡åŒ–å¤„ç†
    print(f'[time cost]{time_cost_hms(time.perf_counter() - starttime)} â€” load point cloud to total_pointcloud.')
    return data, total_pointcloud, clusters_pointcloud, cluster_id_dict


def load_pointcloud_QFACET(data_path):
    """
    è¯»å–â€œæ•°æ®æ ¼å¼2â€ï¼ˆå«è¡¨å¤´ã€tabåˆ†éš”ï¼‰æ–‡ä»¶ï¼Œå¹¶è¿”å›ï¼š
      data: (N,9) å…¼å®¹æ—§æ¥å£çš„ ndarrayï¼Œåˆ—ä¸º [X,Y,Z, js, cl, A,B,C,D]
      total_pointcloud: PointCloud.RockPointCloud
      clusters_pointcloud: dict[(js,cl)] -> RockPointCloud
      cluster_id_dict: dict[(js,cl)] -> combined_cluster_id
    è§„åˆ™ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š
      - æ­£å¸¸ç°‡(js!=-1, cl!=-1)ï¼šcombined_cluster_id æŒ‰å‡ºç°é¡ºåºè¿ç»­ç¼–å·ï¼ˆä¸åŸé€»è¾‘ç›¸åŒï¼‰
      - js==-1ï¼šcl ç”± facet_id å»é‡åæ˜ å°„åˆ° 0,1,2,...ï¼›combined_cluster_id = facet_id
                 è‹¥ facet_id æ— æ•ˆ(<0) åˆ™è¯¥ç‚¹ä»è®°ä¸ºå™ªå£°ï¼ˆcombined_cluster_id=-1ï¼‰
    """
    starttime = time.perf_counter()

    # 1) è¯»è¡¨ + åŸºç¡€æ ¡éªŒ
    df = pd.read_csv(data_path, sep="\t")
    need = ["X", "Y", "Z", "cluster_id", "subcluster_id", "facet_id", "A", "B", "C", "D"]
    miss = [c for c in need if c not in df.columns]
    if miss:
        raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {miss}")

    # 2) ç±»å‹è§„èŒƒï¼ˆæ— æ•ˆä¸€å¾‹ç½® -1 / NaNâ†’-1ï¼‰
    for col in ["X", "Y", "Z", "A", "B", "C", "D"]:
        df[col] = pd.to_numeric(df[col], errors="coerce")
    df[["cluster_id", "subcluster_id", "facet_id"]] = df[["cluster_id", "subcluster_id", "facet_id"]].apply(
        lambda s: pd.to_numeric(s, errors="coerce").fillna(-1).astype(int)
    )

    # 3) ä¸º js==-1 çš„ç‚¹ï¼ŒæŒ‰ facet_id ç”Ÿæˆæ–°çš„ clï¼ˆ0..M-1ï¼‰
    neg_mask = (df["cluster_id"] == -1)
    valid_facet = df.loc[neg_mask & (df["facet_id"] >= 0), "facet_id"]
    uniq_facet = pd.unique(valid_facet)
    facet2cl = {int(fid): i for i, fid in enumerate(uniq_facet)}  # facet_id -> new_cl

    # ä»…è¦†ç›– js==-1 çš„ clï¼›å…¶ä»–ä¿æŒåŸ cl å€¼
    df.loc[neg_mask, "subcluster_id"] = df.loc[neg_mask, "facet_id"].map(facet2cl).fillna(-1).astype(int)

    # 4) ç”Ÿæˆ cluster_id_dict
    #    - æ­£å¸¸ç°‡ï¼šè¿ç»­ç¼–å·ï¼ˆä¿æŒå’Œä½ å½“å‰å®ç°ä¸€è‡´ï¼‰:contentReference[oaicite:2]{index=2}
    #    - js==-1ï¼š(-1, new_cl) -> facet_id
    pairs_pos = list(
        df.loc[(df["cluster_id"] != -1) & (df["subcluster_id"] != -1), ["cluster_id", "subcluster_id"]]
        .drop_duplicates().itertuples(index=False, name=None)
    )
    cluster_id_dict = {key: idx for idx, key in enumerate(pairs_pos)}  # æ­£å¸¸ç°‡

    # ä¸º js==-1 çš„æ¯ä¸ªæ˜ å°„ï¼ŒæŒ‡å®š combined_cluster_id = facet_idï¼ˆä¸å ç”¨è¿ç»­ç¼–å·ï¼‰
    for fid, new_cl in facet2cl.items():
        cluster_id_dict[(-1, new_cl)] = int(fid)

    # 5) æ„å»ºå®¹å™¨
    clusters_pointcloud = {key: PointCloud.RockPointCloud() for key in cluster_id_dict.keys()}
    total_pointcloud = PointCloud.RockPointCloud()

    # 6) å†™å…¥ç‚¹ï¼ˆå« combined_cluster_id çš„æ–°è§„åˆ™ï¼‰
    X = df["X"].to_numpy(float);
    Y = df["Y"].to_numpy(float);
    Z = df["Z"].to_numpy(float)
    js = df["cluster_id"].to_numpy(int);
    cl = df["subcluster_id"].to_numpy(int)
    a = df["A"].to_numpy(float);
    b = df["B"].to_numpy(float);
    c = df["C"].to_numpy(float);
    d = df["D"].to_numpy(float)
    fid = df["facet_id"].to_numpy(int)
    R = G = B = 0

    n = len(df)
    for i in range(n):
        j = int(js[i]);
        k = int(cl[i])
        if j == -1:
            # å…³é”®ï¼šjs==-1 â†’ combined_cluster_id = facet_idï¼ˆè‹¥ facet_id æ— æ•ˆåˆ™ä»è®°å™ªå£°ï¼‰
            ccid = int(fid[i]) if int(fid[i]) >= 0 else -1
        elif k == -1:
            ccid = -1
        else:
            ccid = cluster_id_dict[(j, k)]

        total_pointcloud.add(float(X[i]), float(Y[i]), float(Z[i]),
                             int(i), j, k, int(ccid), R, G, B,
                             float(a[i]), float(b[i]), float(c[i]), float(d[i]))

        if ccid != -1:
            clusters_pointcloud[(j, k)].add(float(X[i]), float(Y[i]), float(Z[i]),
                                            int(i), j, k, int(ccid), R, G, B,
                                            float(a[i]), float(b[i]), float(c[i]), float(d[i]))

    print(f"[time cost]{time_cost_hms(time.perf_counter() - starttime)} - load point cloud (format2).")

    # 7) å…¼å®¹æ—§ ndarrayï¼ˆåˆ—é¡ºåºä¸åŸç‰ˆä¸€è‡´ï¼‰:contentReference[oaicite:3]{index=3}
    data = np.column_stack([X, Y, Z, js, cl, a, b, c, d]).astype(np.float64)
    return data, total_pointcloud, clusters_pointcloud, cluster_id_dict


def get_clusters(data, total_pointcloud, clusters_pointcloud, cluster_id_dict):
    '''

    :param data:
    :param total_pointcloud:
    :param clusters_pointcloud: dict{(int, int):PointCloud.RockPointCloud()},é”®ä¸º (js, cl),å€¼ä¸ºclusterå¯¹åº”çš„ç‚¹äº‘ã€‚
    :param cluster_id_dict:
    :return: clustersç±»
    '''
    testset = [(4, 39), (2, 7), (1, 37), (5, 81), (2, 87), (4, 48), (6, 80), (2, 196)]  # ]]]]]]]]]]]]]]]]]]]]]]]]]

    starttime = time.perf_counter()
    # éå†dataçš„æ¯è¡Œç‚¹æ•°æ®ï¼Œå°†ç‚¹åˆ†é…åˆ°ä¸cluster_id=(js, cl)å¯¹åº”çš„listå®¹å™¨ä¸­
    for point_idx, row in enumerate(data):
        js = int(row[3])  # ç¬¬ 4 åˆ—
        cl = int(row[4])  # ç¬¬ 5 åˆ—
        if cl == -1:
            continue
        clusters_pointcloud[(js, cl)].append(total_pointcloud.points[point_idx])

    # ä»clusters_pointcloudä¸­æ„å»ºclusterç±»
    clusters = Cluster.Clusters()
    for key, cluster_id in cluster_id_dict.items():
        # # test part
        # if key not in testset:
        #     continue

        # get point cloud and plane_params
        rock_points = clusters_pointcloud[key]
        plane_params = rock_points.points[0].plane_paras

        # initial inlier_ratio
        inlier_ratio = 0

        # append cluster object
        clusters.add(key[0], key[1], cluster_id, rock_points, plane_params, inlier_ratio)

    print(f'[time cost]{time_cost_hms(time.perf_counter() - starttime)} â€” create clusters object of RockSeg3D.')
    return clusters


def ransac_planarity_filter(clusters):
    threshold_inliers_ratio = 0.4
    threshold_inliers_number = 50
    test_scale_with_factor = []
    for cluster in clusters.clusters:

        # get the coordinates of point cloud
        points_coord = np.asarray([pt.coord for pt in cluster.rock_points.points])

        # get the scale of point cloud
        starttime1 = time.perf_counter()
        scale_with_factor, scale = estimate_adaptive_threshold(points_coord, scale_factor=0.05)
        # print(f'[time cost]{time_cost_hms(time.perf_counter() - starttime1)} â€” estimate_adaptive_threshold.')

        # test
        test_scale_with_factor.append(scale_with_factor)
        print(scale_with_factor)

        # åŸºäºç‚¹äº‘å°ºåº¦ï¼Œè‡ªé€‚åº”è°ƒæ•´RANSACå¹³é¢æ‹Ÿåˆçš„å®¹å·®é˜ˆå€¼distance_threshold
        starttime2 = time.perf_counter()
        inliers, outliers, inlier_mask, plane_params = fit_plane_ransac(points_coord,
                                                                        distance_threshold=scale_with_factor)
        # print(f'[time cost]{time_cost_hms(time.perf_counter() - starttime2)} â€” fit_plane_ransac.')

        # æ£€æŸ¥å†…ç‚¹ç‡é˜ˆå€¼å’Œå†…ç‚¹æ•°é‡é˜ˆå€¼
        inlier_ratio = len(inliers) / (len(inliers) + len(outliers))
        if inlier_ratio <= threshold_inliers_ratio or len(inliers) <= threshold_inliers_number:
            cluster.valid = False
            print(
                f"False!  å†…ç‚¹æ•°: {len(inliers)}, å¤–ç‚¹æ•°: {len(outliers)}, å†…ç‚¹ç‡: {inlier_ratio},{cluster.joint_id}-{cluster.joint_cluster_id} ")
            continue  # è·³è¿‡éå¹³é¢æˆ–å°å¹³é¢
        else:
            print(
                f"å†…ç‚¹æ•°: {len(inliers)}, å¤–ç‚¹æ•°: {len(outliers)}, å†…ç‚¹ç‡: {inlier_ratio},{cluster.joint_id}-{cluster.joint_cluster_id} ")

        # # calculate planarity score of cluster points
        # planarity_score, uniform_score = check_cluster_planarity(points_coord, k=30)
        # scores = compute_planarity_score(points_coord, k=30)
        # # åˆ†ç¦»å†…å¤–ç‚¹ï¼ˆ20% æœ€ä¸å¹³é¢ä¸ºå¤–ç‚¹ï¼‰
        # inliers, outliers = separate_inliers_outliers(points_coord, scores, method="quantile", threshold=0.2)
        # print(f'[time cost]{time_cost_hms(time.perf_counter() - starttime0)} â€” remove outlier.')

        # å¯è§†åŒ–
        # pcd_in = o3d.geometry.PointCloud()
        # pcd_in.points = o3d.utility.Vector3dVector(inliers)
        # pcd_in.paint_uniform_color([0, 1, 0])  # ç»¿è‰²
        # pcd_out = o3d.geometry.PointCloud()
        # pcd_out.points = o3d.utility.Vector3dVector(outliers)
        # pcd_out.paint_uniform_color([1, 0, 0])  # çº¢è‰²
        # o3d.visualization.draw_geometries([pcd_in, pcd_out])

        # update cluster attributes
        # print(inlier_mask)
        # print(np.all(~inlier_mask))
        cluster.inlier_mask = inlier_mask
        cluster.inlier_number = len(inliers)
        cluster.inlier_ratio = inlier_ratio
        cluster.valid = True
        cluster.centroid = cluster.get_centroid()
        cluster.plane_params = np.asarray(plane_params, dtype=np.float64)
        cluster.normal = plane_params[:3] / np.linalg.norm(plane_params[:3])

    # test

    return clusters


def estimate_adaptive_threshold(points, scale_factor=0.02):
    """
    åŸºäºPCAä¸»è½´å°ºåº¦åŠ¨æ€è®¾ç½®æ‹Ÿåˆå®¹å·®é˜ˆå€¼ï¼ˆä¾‹å¦‚ç”¨äºRANSACï¼‰ã€‚

    å‚æ•°:
        points (np.ndarray): shape (N, 3) çš„ä¸‰ç»´ç‚¹äº‘
        scale_factor (float): ä¸ç‚¹äº‘å°ºåº¦æˆæ¯”ä¾‹çš„ç³»æ•°ï¼ˆé»˜è®¤2%ï¼‰

    è¿”å›:
        distance_threshold (float): å¹³é¢æ‹Ÿåˆè·ç¦»å®¹å·®
    """
    # assert points.ndim == 2 and points.shape[1] == 3, "è¾“å…¥å¿…é¡»æ˜¯(N, 3)ä¸‰ç»´æ•°ç»„"
    #
    # # å»ä¸­å¿ƒåŒ–
    # centered = points - np.mean(points, axis=0)
    #
    # # PCA ä¸»è½´å°ºåº¦ï¼ˆåæ–¹å·®çŸ©é˜µç‰¹å¾å€¼ï¼‰
    # cov = np.cov(centered.T)
    # eigvals = np.linalg.eigvalsh(cov)  # å·²æ’åºï¼ˆå‡åºï¼‰
    #
    # # ä½¿ç”¨æœ€å¤§ç©ºé—´å°ºåº¦ä½œä¸ºå‚è€ƒ
    # first_max_extent = np.sqrt(eigvals[-1])
    # second_max_extent = np.sqrt(eigvals[-2])
    #
    # scale_area = first_max_extent * second_max_extent  # çŸ©å½¢é¢ç§¯

    # åŸºäºaabbå¤–åŒ…ç›’çš„å¯¹è§’çº¿é•¿åº¦æ¥è¯„ä¼°ç‚¹äº‘é›†çš„å°ºå¯¸
    min_corner = np.min(points, axis=0)
    max_corner = np.max(points, axis=0)
    diag_len = np.linalg.norm(max_corner - min_corner)

    tolerance = a * log(1 + b * diag_len)  # a,bä¸ºç»éªŒå‚æ•°

    return tolerance, diag_len

    # # é˜ˆå€¼ä¸å°ºåº¦æˆæ­£æ¯”
    # distance_threshold = scale_area * scale_factor
    # return distance_threshold, scale_area


def fit_plane_ransac(point_array, distance_threshold=0.01, ransac_n=3, num_iterations=1000):
    '''
    ä½¿ç”¨ RANSAC æ‹Ÿåˆç©ºé—´å¹³é¢å¹¶è¿”å›å¹³é¢å‚æ•°ã€å†…ç‚¹å’Œå¤–ç‚¹   inlier_maskğŸ˜¢

    å‚æ•°:
    - point_array: np.ndarray, shape (N, 3)ï¼Œè¾“å…¥çš„ç‚¹äº‘åæ ‡
    - distance_threshold: floatï¼Œå†…ç‚¹çš„æœ€å¤§è·ç¦»é˜ˆå€¼
    - ransac_n: intï¼ŒRANSAC æ¯æ¬¡æ‹Ÿåˆæ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°ï¼ˆæ‹Ÿåˆå¹³é¢ä¸º3ï¼‰
    - num_iterations: intï¼ŒRANSAC æœ€å¤§è¿­ä»£æ¬¡æ•°

    è¿”å›:
    - plane_model: list[float], [a, b, c, d] å¹³é¢æ–¹ç¨‹ ax + by + cz + d = 0
    - inliers: np.ndarray, shape (M, 3)ï¼Œæ‹Ÿåˆå¹³é¢ä¸Šçš„å†…ç‚¹
    - outliers: np.ndarray, shape (N-M, 3)ï¼Œä¸åœ¨å¹³é¢ä¸Šçš„å¤–ç‚¹
    '''

    # è½¬ä¸º Open3D ç‚¹äº‘å¯¹è±¡
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(point_array)

    # RANSAC æ‹Ÿåˆå¹³é¢
    plane_params, inlier_idxs = pcd.segment_plane(
        distance_threshold=distance_threshold,
        ransac_n=ransac_n,
        num_iterations=num_iterations
    )  # inlier_idxsä¸­çš„æ¯ä¸ªæ•°æ®æ˜¯point_arrayçš„ç´¢å¼•ï¼ŒæŒ‡å‘äº†å†…ç‚¹

    # å°†ç´¢å¼•è½¬æ¢ä¸ºmask
    inlier_mask = np.zeros(len(point_array), dtype=bool)
    inlier_mask[inlier_idxs] = True

    # æå–å†…ç‚¹å’Œå¤–ç‚¹
    inliers = point_array[inlier_mask]
    outliers = point_array[~inlier_mask]

    return inliers, outliers, inlier_mask, plane_params


def check_cluster_planarity(points, k=30):
    '''
    å¯¹æ•´ä¸ªç‚¹é›†è®¡ç®—æ•´ä½“å¹³é¢æ€§è¯„åˆ†
    è¾“å…¥ï¼š
        points: (N, 3) np.ndarray
    è¾“å‡ºï¼š
        planarity_score: floatï¼Œè¶Šå°è¶Šæ¥è¿‘å¹³é¢
        normal: æ³•å‘é‡ (3,)
    '''
    pts_centered = points - points.mean(axis=0)
    cov = pts_centered.T @ pts_centered
    eigvals, eigvecs = np.linalg.eigh(cov)
    eigvals = np.sort(eigvals)[::-1]  # Î»1 â‰¥ Î»2 â‰¥ Î»3

    planarity_score = eigvals[2] / eigvals.sum()  # è¶Šå°è¶Šå¹³é¢
    # normal = eigvecs[:, 0]  # æœ€å°ç‰¹å¾å€¼å¯¹åº”æ³•å‘

    """
    è®¡ç®—æ¯ä¸ªç‚¹çš„å±€éƒ¨ç‚¹å¯†åº¦ï¼ˆké‚»åŸŸåè·ç¦»å‡å€¼ï¼‰ã€‚
    è¾“å…¥ï¼š
        points: (N,3) ndarrayï¼Œç‚¹é›†
        k: é‚»åŸŸç‚¹æ•°é‡
    è¾“å‡ºï¼š
        densities: (N,) ndarrayï¼Œå¯†åº¦æŒ‡æ ‡ï¼ˆæ•°å€¼è¶Šå¤§å¯†åº¦è¶Šä½ï¼‰
    """
    from sklearn.neighbors import NearestNeighbors
    nbrs = NearestNeighbors(n_neighbors=k + 1, algorithm='auto').fit(points)
    distances, _ = nbrs.kneighbors(points)
    # å¿½ç•¥è‡ªèº«ç‚¹
    avg_distance = np.mean(distances[:, 1:], axis=1)
    density = 1 / (avg_distance + 1e-8)

    # score: floatï¼Œå¯†åº¦å‡åŒ€æ€§å¾—åˆ†ï¼ˆå€¼è¶Šå°è¡¨ç¤ºè¶Šå‡åŒ€ï¼‰
    uniform_score = np.std(density) / np.mean(density)

    return planarity_score, uniform_score


def compute_planarity_score(points, k=30):
    '''
    è®¡ç®—æ¯ä¸ªç‚¹çš„å¹³é¢æ€§å¾—åˆ†ï¼ˆÎ»â‚‚ / Î»â‚ƒï¼‰
    è¾“å…¥:
        points: (N,3) ç‚¹äº‘åæ ‡
        k: é‚»åŸŸå¤§å°
    è¾“å‡º:
        planarity_scores: (N,) arrayï¼Œè¶Šå¤§è¶Šå¹³é¢
    '''
    from sklearn.neighbors import NearestNeighbors
    scores = np.zeros(len(points))
    nbrs = NearestNeighbors(n_neighbors=k).fit(points)
    indices = nbrs.kneighbors(return_distance=False)

    for i, idx in enumerate(indices):
        neighbors = points[idx] - points[idx].mean(axis=0)
        cov = neighbors.T @ neighbors
        eigvals = np.sort(np.linalg.eigvalsh(cov))[::-1]  # Î»1 â‰¥ Î»2 â‰¥ Î»3
        if eigvals[1] != 0:
            scores[i] = (eigvals[1] - eigvals[2]) / eigvals[0]  # å¯æ›¿æ¢ä¸º Î»â‚‚ / Î»â‚ƒ æˆ–å…¶ä»–æŒ‡æ ‡
        else:
            scores[i] = 0

    nbrs = NearestNeighbors(n_neighbors=k + 1, algorithm='auto').fit(points)
    distances, _ = nbrs.kneighbors(points)
    avg_distance = np.mean(distances[:, 1:], axis=1)
    densitys = 1 / (avg_distance + 1e-8)

    return scores + densitys


def separate_inliers_outliers(points, planarity_scores, method="quantile", threshold=0.2):
    '''
    æ ¹æ®å¹³é¢æ€§å¾—åˆ†åˆ†ç¦»å†…ç‚¹ä¸å¤–ç‚¹

    è¾“å…¥ï¼š
        points: (N,3) ç‚¹äº‘åæ ‡
        planarity_scores: (N,) å¹³é¢æ€§å¾—åˆ†
        method: "quantile"ï¼ˆåˆ†ä½æ•°ï¼‰æˆ– "std"ï¼ˆæ ‡å‡†å·®ï¼‰
        threshold: è‹¥ method="quantile"ï¼Œåˆ™è¡¨ç¤ºåˆ†ä½æ•°ï¼ˆå¦‚ 0.2ï¼‰ï¼›è‹¥ method="std"ï¼Œè¡¨ç¤ºæ ‡å‡†å·®å€æ•°ï¼ˆå¦‚ 1.0ï¼‰

    è¾“å‡ºï¼š
        inlier_points: å¹³é¢æ€§è¾ƒé«˜çš„ç‚¹
        outlier_points: éå¹³é¢åŒºåŸŸæˆ–å¼‚å¸¸ç‚¹
    '''
    if method == "quantile":
        cutoff = np.quantile(planarity_scores, threshold)
        mask = planarity_scores >= cutoff
    elif method == "std":
        mean = np.mean(planarity_scores)
        std = np.std(planarity_scores)
        mask = planarity_scores >= (mean - threshold * std)
    else:
        raise ValueError("Unsupported method")

    return points[mask], points[~mask]


def get_discontinuitys(clusters):
    def npget_dip_dir(np_clusters_normal):
        '''
        'npget' means using numpy tensor to facilitate the process.

        :param np_clusters_planeparas: numpy array with shape: (N, 3)ï¼Œå½’ä¸€åŒ–åçš„A_nol, B_nol, C_nol
        :return: dip_direction, direction: numpy array with shape: (N, 2)ï¼Œå€¾è§’ã€å€¾å‘
        '''
        a = np_clusters_normal[:, 0]
        b = np_clusters_normal[:, 1]
        c = np_clusters_normal[:, 2]
        # Dï¼šæ˜¯å¹³é¢ä¸åŸç‚¹ä½ç½®å…³ç³»æœ‰å…³çš„åç§»é‡ï¼Œä¸å½±å“æ–¹å‘ï¼Œå› æ­¤ä¸å‚ä¸å•ä½åŒ–

        # å€¾è§’ dip = arccos(|c|)ï¼Œå•ä½ä¸ºåº¦
        dips = np.degrees(np.arccos(np.abs(c)))  # ç»“æœèŒƒå›´ï¼š[0, 90]

        # å€¾å‘ strike = atan2(-b, a)ï¼Œç»“æœèŒƒå›´ [0, 360)
        strikes = np.degrees(np.arctan2(-b, a))
        strikes = np.where(strikes < 0, strikes + 360, strikes)

        # è¾“å‡º shape æ ¡éªŒï¼ˆå¯é€‰ï¼‰
        print('A_nor.shape:', a.shape)
        print('dips.shape:', dips.shape)

        # åˆå¹¶è¾“å‡º
        dip_dir = np.stack([dips, strikes], axis=1)  # (N, 2)
        return dip_dir  # æ¯ä¸€è¡Œæ˜¯ [dip, strike]

    starttime = time.perf_counter()

    # calculate dip dir with numpy tensor
    np_clusters_normal = np.asarray([cluster.normal for cluster in clusters.clusters])
    np_dip_dir = npget_dip_dir(np_clusters_normal)

    discontinuitys = Discontinuity.Discontinuitys()
    for idx, cluster in enumerate(clusters.clusters):
        disc = Discontinuity.Discontinuity.from_cluster(cluster)
        disc.dip = np_dip_dir[idx, 0]
        disc.strike = np_dip_dir[idx, 1]
        disc.type = 'free_surface'
        discontinuitys.add(disc)

    print(f'[time cost]{time_cost_hms(time.perf_counter() - starttime)} â€” create discontinuitys from clusters.')
    return discontinuitys


def time_cost_hms(seconds: float) -> str:
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = seconds % 60
    return f"{h} h {m} min {s:.2f} sec"


###################################################
###################################################
###################################################
def get_cluster_id(cluster_pointcloud):
    cluster_id_by_key = {}
    cluster_key_by_id = {}
    for id, (key, point) in enumerate(cluster_pointcloud.items()):
        cluster_id_by_key[key] = id
        cluster_key_by_id[id] = key

    # with open(save_path_1, "wb") as f:
    #     pickle.dump(cluster_key_to_ids, f)
    #
    # with open(save_path_2, "wb") as f:
    #     pickle.dump(cluster_ids_to_key, f)

    return cluster_id_by_key, cluster_key_by_id


def get_cluster_planeparas(data, cluster_point, save_path):
    cluster_planeparas = {}
    for key, point in cluster_point.items():
        cluster_planeparas[key] = data[point[0], 5:9]

    with open(save_path, "wb") as f:
        pickle.dump(cluster_planeparas, f)

    return cluster_planeparas


def get_plane_params(data, cluster_point, js_val, cl_val):
    # æ‰¾åˆ°æ‰€æœ‰åŒ¹é…çš„ç´¢å¼•
    indices = cluster_point.get((js_val, cl_val), [])
    if indices:
        i = indices[0]  # å–ç¬¬ä¸€ä¸ªåŒ¹é…ç´¢å¼•
        return data[i, 5:9]  # ç¬¬6-9åˆ—ä¸º a, b, c, d
    else:
        return None


def get_strike_dip(A, B, C):
    # å•ä½åŒ–æ³•å‘é‡
    norm = np.sqrt(A ** 2 + B ** 2 + C ** 2)
    a, b, c = A / norm, B / norm, C / norm

    # å€¾è§’ï¼ˆDipï¼‰
    dip = np.degrees(np.arccos(abs(c)))  # æ³¨æ„å–absä¿è¯æ­£å€¼

    # å€¾å‘ï¼ˆStrikeï¼‰ï¼Œæ³¨æ„ atan2(-B, A)
    strike = np.degrees(np.arctan2(-b, a))
    if strike < 0:
        strike += 360

    return strike, dip


def get_trace_plane(cluster_point: dict, dip_dict: dict, save_path):
    """
    ç»™å®šä¸‰ç»´ç‚¹åŠå…¶æ³•å‘å¹³é¢å‚æ•°ï¼Œå°†ç‚¹æŠ•å½±è‡³å¹³é¢åï¼Œä½¿ç”¨â€œä¸¤æ¬¡æœ€è¿œç‚¹æœç´¢â€æ³•æ‰¾åˆ°traceçš„ä¸¤ä¸ªç«¯ç‚¹ã€‚

    :param cluster_point : dictï¼Œé”®ä¸º(js, cl)ï¼Œå€¼ä¸ºç‚¹ç´¢å¼•åˆ—è¡¨
    :param dip_dict       : dictï¼ŒåŒ…å«æ³•å‘é‡ä¿¡æ¯ï¼ˆA,B,Cï¼‰
    :param save_path      : è·¯å¾„ï¼Œä¿å­˜trace_dictç»“æœ
    :return: trace_dict : dictï¼Œé”®ä¸º(js, cl)ï¼Œå€¼ä¸ºå­—å…¸ï¼Œå«é•¿åº¦å’Œç«¯ç‚¹åæ ‡
            trace_dict[(js, cl)] = {
            'length': max_len,
            'start': coord_start.tolist(),
            'end': coord_end.tolist(),
            'start_idx': int(idx_start),
            'end_idx': int(idx_end)
        }
    """

    def project_points_to_plane(points, normal):
        """
        å°†ä¸‰ç»´ç‚¹æŠ•å½±åˆ°ç»™å®šæ³•å‘é‡å®šä¹‰çš„å¹³é¢ä¸Š
        è¿”å›æŠ•å½±åçš„äºŒç»´åæ ‡å’Œå¹³é¢å±€éƒ¨åæ ‡ç³»åŸºå‘é‡ï¼ˆu, vï¼‰
        """
        normal = normal / np.linalg.norm(normal)
        # æ‰¾ä¸€ä¸ªä¸å¹³è¡Œäºæ³•å‘çš„å‘é‡æ„é€ å±€éƒ¨å¹³é¢åæ ‡ç³»
        arbitrary = np.array([1, 0, 0]) if abs(normal[0]) < 0.9 else np.array([0, 1, 0])
        u = np.cross(normal, arbitrary)
        u /= np.linalg.norm(u)
        v = np.cross(normal, u)
        v /= np.linalg.norm(v)
        projected_2d = np.dot(points, np.vstack([u, v]).T)
        return projected_2d, u, v

    def double_farest_search(projected_2d):
        center = projected_2d.mean(axis=0)
        dists_to_center = np.linalg.norm(projected_2d - center, axis=1)
        idx1 = np.argmax(dists_to_center)  # è¾¹ç¼˜ç‚¹1åœ¨ projected_2d ä¸­çš„ç´¢å¼•

        dists_to_idx1 = np.linalg.norm(projected_2d - projected_2d[idx1], axis=1)
        idx2 = np.argmax(dists_to_idx1)  # è¾¹ç¼˜ç‚¹2

        # è®¡ç®—è·ç¦»
        max_len = np.max(dists_to_idx1)

        idx_start = point_indices[idx1]
        idx_end = point_indices[idx2]
        return max_len, idx_start, idx_end

    trace_dict = {}
    for key, point_indices in cluster_point.items():
        if key not in dip_dict:
            continue
        A_nol, B_nol, C_nol = dip_dict[key][2:5]
        normal = np.array([A_nol, B_nol, C_nol])
        points = data[point_indices, 0:3]  # æœªçŸ¥åŸå› å¯¼è‡´åŸºäºpoint_indicesæå–çš„pointsä¸­éƒ½æ˜¯åŒä¸€ä¸ªç‚¹ï¼Œå¯èƒ½æ˜¯å¤šä¸ªç‚¹éƒ½æ˜¯åŒä¸€ä¸ªåæ ‡

        # print(points)
        # å°†ç‚¹æŠ•å½±åˆ°å¯¹åº”å¹³é¢
        projected_2d, u, v = project_points_to_plane(points, normal)

        if len(projected_2d) < 2:
            print('æ— æ³•è®¡ç®—è·ç¦»')
            continue  # æ— æ³•è®¡ç®—è·ç¦»

        # ä½¿ç”¨â€œä¸¤æ¬¡æœ€è¿œç‚¹æœç´¢â€æ³•æ‰¾åˆ°traceçš„ä¸¤ä¸ªç«¯ç‚¹
        max_len, idx_start, idx_end = double_farest_search(projected_2d)
        if max_len == 0:
            # print(f'max_len:{max_len}       {point_indices}')
            continue
            # print(point_indices)
            # print(points)
            # print(u, v)
            # print(projected_2d)
        if max_len > 1:
            print(f'max_len:{max_len}')

        coord_start = data[idx_start, 0:3]
        coord_end = data[idx_end, 0:3]

        trace_dict[key] = {
            'length': max_len,
            'start': coord_start.tolist(),
            'end': coord_end.tolist(),
            'start_idx': int(idx_start),
            'end_idx': int(idx_end)
        }

    with open(save_path, "wb") as f:
        pickle.dump(trace_dict, f)

    return trace_dict


# ================================export
def write_ply_with_edges(filename, points, edges):
    """
    è‡ªå®šä¹‰å†™å…¥ PLY æ–‡ä»¶ï¼ŒåŒ…å« vertex å’Œ edgeï¼ˆlineï¼‰

    Parameters
    ----------
    filename : str
        è¾“å‡º ply æ–‡ä»¶å
    points : (N, 3) np.ndarray
        é¡¶ç‚¹æ•°ç»„ï¼Œfloat32
    edges : (M, 2) np.ndarray
        æ¯æ¡çº¿ç”±ä¸¤ä¸ªç‚¹ç´¢å¼•ç»„æˆï¼Œint32
    """
    num_vertices = points.shape[0]
    num_edges = edges.shape[0]

    with open(filename, 'wb') as f:
        # ===== å†™å…¥ PLY å¤´éƒ¨ =====
        header = f"""ply
format binary_little_endian 1.0
element vertex {num_vertices}
property float x
property float y
property float z
element edge {num_edges}
property int vertex1
property int vertex2
end_header
"""
        f.write(header.encode('utf-8'))

        # ===== å†™å…¥ vertex åæ ‡æ•°æ® (float32) =====
        for pt in points:
            f.write(struct.pack('<fff', *pt))

        # ===== å†™å…¥ edge æ•°æ®ï¼ˆæ¯æ¡çº¿æ˜¯ä¸¤ä¸ªé¡¶ç‚¹ç´¢å¼•ï¼‰ =====
        for edge in edges:
            f.write(struct.pack('<ii', *edge))


def export_trace_to_ply(trace_dict, save_path):
    """
    å°† trace_dict ä¸­çš„æ¯æ¡è¿¹çº¿ï¼ˆstart, endï¼‰å¯¼å‡ºä¸ºåŒ…å« line ç±»å‹çš„ PLY æ–‡ä»¶

    Parameters
    ----------
    trace_dict : dict
        æ¯æ¡è¿¹çº¿åŒ…å« 'start', 'end' åæ ‡
    filename : str
        è¾“å‡º ply æ–‡ä»¶è·¯å¾„
    """
    points = []
    edges = []

    for trace in trace_dict.values():
        start = trace['start']
        end = trace['end']
        idx_start = len(points)
        idx_end = idx_start + 1
        points.append(start)
        points.append(end)
        edges.append([idx_start, idx_end])

    points = np.array(points, dtype=np.float64)
    edges = np.array(edges, dtype=np.int32)

    write_ply_with_edges(save_path, points, edges)
